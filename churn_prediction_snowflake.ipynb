{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Churn Prediction with LSTM + Attention\n",
    "## Using Real Snowflake Data with 12-Month Lookback\n",
    "\n",
    "This notebook trains a churn prediction model using:\n",
    "- **Data Source**: Snowflake tables (PHONE_USAGE_DATA, ACCOUNT_ATTRIBUTES_MONTHLY, CHURN_RECORDS)\n",
    "- **Model**: LSTM with Attention mechanism\n",
    "- **Features**: 12-month usage sequences\n",
    "- **Target**: Predict if account will churn\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.metrics import (\n",
    "    auc, roc_curve, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, precision_recall_curve,\n",
    "    classification_report\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Snowflake\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark.functions import col, lit, count, sum as spark_sum\n",
    "from snowflake.snowpark.types import StructType, StructField, StringType, FloatType, IntegerType\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully\")\n",
    "print(f\"âœ“ PyTorch version: {torch.__version__}\")\n",
    "print(f\"âœ“ CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"âœ“ Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Connect to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get active Snowflake session\n",
    "session = get_active_session()\n",
    "\n",
    "# Set database and schema to MY_DATABASE.PUBLIC\n",
    "session.use_database(\"MY_DATABASE\")\n",
    "session.use_schema(\"PUBLIC\")\n",
    "\n",
    "print(\"âœ“ Snowflake session active\")\n",
    "print(f\"  Database: {session.get_current_database()}\")\n",
    "print(f\"  Schema: {session.get_current_schema()}\")\n",
    "print(f\"  Warehouse: {session.get_current_warehouse()}\")\n",
    "print(f\"  Role: {session.get_current_role()}\")\n",
    "\n",
    "# Verify tables exist\n",
    "print(\"\\nâœ“ Verifying tables exist...\")\n",
    "tables_to_check = [\"PHONE_USAGE_DATA\", \"ACCOUNT_ATTRIBUTES_MONTHLY\", \"CHURN_RECORDS\"]\n",
    "for table in tables_to_check:\n",
    "    count = session.table(f\"MY_DATABASE.PUBLIC.{table}\").count()\n",
    "    print(f\"  {table}: {count:,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data from Snowflake Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Snowflake tables in MY_DATABASE.PUBLIC\n",
    "print(\"Loading data from MY_DATABASE.PUBLIC...\")\n",
    "\n",
    "# 1. Usage data\n",
    "usage_df = session.table(\"MY_DATABASE.PUBLIC.PHONE_USAGE_DATA\").to_pandas()\n",
    "usage_df['MONTH'] = pd.to_datetime(usage_df['MONTH'])\n",
    "print(f\"âœ“ PHONE_USAGE_DATA: {len(usage_df):,} rows\")\n",
    "\n",
    "# 2. Account attributes\n",
    "account_df = session.table(\"MY_DATABASE.PUBLIC.ACCOUNT_ATTRIBUTES_MONTHLY\").to_pandas()\n",
    "account_df['MONTH'] = pd.to_datetime(account_df['MONTH'])\n",
    "print(f\"âœ“ ACCOUNT_ATTRIBUTES_MONTHLY: {len(account_df):,} rows\")\n",
    "\n",
    "# 3. Churn records\n",
    "churn_df = session.table(\"MY_DATABASE.PUBLIC.CHURN_RECORDS\").to_pandas()\n",
    "churn_df['CHURN_DATE'] = pd.to_datetime(churn_df['CHURN_DATE'])\n",
    "print(f\"âœ“ CHURN_RECORDS: {len(churn_df):,} rows\")\n",
    "\n",
    "print(f\"\\nData date range: {usage_df['MONTH'].min()} to {usage_df['MONTH'].max()}\")\n",
    "print(f\"Unique accounts: {usage_df['USERID'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display data schema\n",
    "print(\"\\n=== PHONE_USAGE_DATA Schema ===\")\n",
    "print(usage_df.dtypes)\n",
    "\n",
    "print(\"\\n=== Sample Usage Data ===\")\n",
    "print(usage_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering - Create 12-Month Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_churn_sequences(usage_df, account_df, churn_df, max_lookback=12):\n",
    "    \"\"\"\n",
    "    Create time-series sequences for churn prediction.\n",
    "    \n",
    "    For each account:\n",
    "    - Extract up to 12 months of usage history\n",
    "    - Create feature vectors for each month\n",
    "    - Label with churn status\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: account_id, sequence (list of feature vectors), churn_label, seq_length\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Creating 12-Month Sequences for Churn Prediction\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Get list of churned accounts\n",
    "    churned_accounts = set(churn_df['USERID'].unique())\n",
    "    print(f\"\\nChurned accounts: {len(churned_accounts):,}\")\n",
    "    \n",
    "    # Get all unique accounts\n",
    "    all_accounts = usage_df['USERID'].unique()\n",
    "    print(f\"Total accounts: {len(all_accounts):,}\")\n",
    "    \n",
    "    sequences = []\n",
    "    \n",
    "    for account_id in tqdm(all_accounts, desc=\"Processing accounts\"):\n",
    "        # Get usage history for this account\n",
    "        account_usage = usage_df[usage_df['USERID'] == account_id].sort_values('MONTH')\n",
    "        \n",
    "        # Skip if less than 3 months of data\n",
    "        if len(account_usage) < 3:\n",
    "            continue\n",
    "        \n",
    "        # Take last 12 months (or all available if less)\n",
    "        account_usage = account_usage.tail(max_lookback)\n",
    "        \n",
    "        # Check if churned\n",
    "        is_churned = 1 if account_id in churned_accounts else 0\n",
    "        \n",
    "        # Extract features for each month\n",
    "        monthly_features = []\n",
    "        \n",
    "        for _, row in account_usage.iterrows():\n",
    "            # Normalize features to [0, 1] range (approximately)\n",
    "            features = [\n",
    "                # Call volume features\n",
    "                min(row['PHONE_TOTAL_CALLS'] / 1000, 1.0),  # Normalized total calls\n",
    "                min(row['PHONE_TOTAL_MINUTES_OF_USE'] / 10000, 1.0),  # Normalized minutes\n",
    "                min(row['VOICE_CALLS'] / 1000, 1.0),  # Voice calls\n",
    "                min(row['FAX_CALLS'] / 100, 1.0),  # Fax calls\n",
    "                \n",
    "                # Call direction features\n",
    "                min(row['PHONE_TOTAL_NUM_INBOUND_CALLS'] / 500, 1.0),\n",
    "                min(row['PHONE_TOTAL_NUM_OUTBOUND_CALLS'] / 500, 1.0),\n",
    "                \n",
    "                # Device usage features\n",
    "                min(row['HARDPHONE_CALLS'] / 500, 1.0),\n",
    "                min(row['SOFTPHONE_CALLS'] / 500, 1.0),\n",
    "                min(row['MOBILE_CALLS'] / 500, 1.0),\n",
    "                \n",
    "                # Engagement feature\n",
    "                min(row['PHONE_MAU'] / 100, 1.0),  # Monthly active users\n",
    "            ]\n",
    "            \n",
    "            monthly_features.append(features)\n",
    "        \n",
    "        sequences.append({\n",
    "            'account_id': account_id,\n",
    "            'sequence': monthly_features,\n",
    "            'churn_label': is_churned,\n",
    "            'seq_length': len(monthly_features)\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    result_df = pd.DataFrame(sequences)\n",
    "    \n",
    "    print(f\"\\nâœ“ Created {len(result_df):,} sequences\")\n",
    "    print(f\"  Churn rate: {result_df['churn_label'].mean():.2%}\")\n",
    "    print(f\"  Avg sequence length: {result_df['seq_length'].mean():.1f} months\")\n",
    "    print(f\"  Min/Max length: {result_df['seq_length'].min()}/{result_df['seq_length'].max()} months\")\n",
    "    \n",
    "    # Show feature statistics\n",
    "    print(f\"\\n  Feature vector size: {len(monthly_features[0])}\")\n",
    "    print(f\"  Features:\")\n",
    "    feature_names = [\n",
    "        'Total Calls (norm)', 'Total Minutes (norm)', 'Voice Calls (norm)', 'Fax Calls (norm)',\n",
    "        'Inbound Calls (norm)', 'Outbound Calls (norm)',\n",
    "        'Hardphone (norm)', 'Softphone (norm)', 'Mobile (norm)',\n",
    "        'MAU (norm)'\n",
    "    ]\n",
    "    for i, name in enumerate(feature_names):\n",
    "        print(f\"    [{i}] {name}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Create sequences\n",
    "sequence_df = create_churn_sequences(usage_df, account_df, churn_df, max_lookback=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample sequences\n",
    "print(\"\\n=== Sample Sequences ===\")\n",
    "print(sequence_df[['account_id', 'seq_length', 'churn_label']].head(10))\n",
    "\n",
    "# Show example sequence\n",
    "print(\"\\n=== Example Sequence (first account) ===\")\n",
    "example_seq = sequence_df.iloc[0]['sequence']\n",
    "print(f\"Account ID: {sequence_df.iloc[0]['account_id']}\")\n",
    "print(f\"Sequence length: {len(example_seq)} months\")\n",
    "print(f\"Churn label: {sequence_df.iloc[0]['churn_label']}\")\n",
    "print(f\"\\nFirst 3 months of features:\")\n",
    "for i, month_features in enumerate(example_seq[:3]):\n",
    "    print(f\"  Month {i+1}: {[f'{x:.3f}' for x in month_features]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Split Data into Train/Val/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, train_ratio=0.7, val_ratio=0.15, random_state=42):\n",
    "    \"\"\"Split data into train, validation, and test sets\"\"\"\n",
    "    # Shuffle data\n",
    "    df = df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    n = len(df)\n",
    "    train_size = int(train_ratio * n)\n",
    "    val_size = int(val_ratio * n)\n",
    "    \n",
    "    train_df = df[:train_size].reset_index(drop=True)\n",
    "    val_df = df[train_size:train_size+val_size].reset_index(drop=True)\n",
    "    test_df = df[train_size+val_size:].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nData split:\")\n",
    "    print(f\"  Train: {len(train_df):,} samples ({len(train_df)/n:.1%})\")\n",
    "    print(f\"  Val:   {len(val_df):,} samples ({len(val_df)/n:.1%})\")\n",
    "    print(f\"  Test:  {len(test_df):,} samples ({len(test_df)/n:.1%})\")\n",
    "    print(f\"\\nChurn rates:\")\n",
    "    print(f\"  Train: {train_df['churn_label'].mean():.2%}\")\n",
    "    print(f\"  Val:   {val_df['churn_label'].mean():.2%}\")\n",
    "    print(f\"  Test:  {test_df['churn_label'].mean():.2%}\")\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "train_df, val_df, test_df = split_data(sequence_df, train_ratio=0.7, val_ratio=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChurnDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for churn prediction with time series data\"\"\"\n",
    "    \n",
    "    def __init__(self, df, max_lookback_window=12):\n",
    "        self.df = df\n",
    "        self.max_lookback_window = max_lookback_window\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get sequence and label\n",
    "        sequence = self.df.iloc[idx]['sequence']\n",
    "        label = self.df.iloc[idx]['churn_label']\n",
    "        \n",
    "        # Convert to tensors\n",
    "        sequence = torch.tensor(sequence, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        \n",
    "        # Pad or truncate sequence to max_lookback_window\n",
    "        seq_len = sequence.shape[0]\n",
    "        if seq_len < self.max_lookback_window:\n",
    "            # Pad with zeros at the beginning\n",
    "            padding = torch.zeros((self.max_lookback_window - seq_len, sequence.shape[1]))\n",
    "            sequence = torch.cat([padding, sequence], dim=0)\n",
    "        elif seq_len > self.max_lookback_window:\n",
    "            # Take last max_lookback_window timesteps\n",
    "            sequence = sequence[-self.max_lookback_window:, :]\n",
    "        \n",
    "        return sequence, label\n",
    "\n",
    "# Configuration\n",
    "MAX_LOOKBACK_WINDOW = 12\n",
    "BATCH_SIZE = 32\n",
    "N_FEATURES = 10  # Number of features per timestep\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ChurnDataset(train_df, MAX_LOOKBACK_WINDOW)\n",
    "val_dataset = ChurnDataset(val_df, MAX_LOOKBACK_WINDOW)\n",
    "test_dataset = ChurnDataset(test_df, MAX_LOOKBACK_WINDOW)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"âœ“ DataLoaders created\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Test data loading\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nSample batch shapes:\")\n",
    "print(f\"  Sequences: {sample_batch[0].shape}\")  # [batch_size, seq_len, n_features]\n",
    "print(f\"  Labels: {sample_batch[1].shape}\")     # [batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Define LSTM with Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMWithAttention(nn.Module):\n",
    "    \"\"\"LSTM model with attention mechanism for sequence classification\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=2, dropout=0.2):\n",
    "        super(LSTMWithAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, \n",
    "            hidden_size, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True, \n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Attention layer\n",
    "        self.attention = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.fc2 = nn.Linear(hidden_size // 2, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, return_attention=False):\n",
    "        # LSTM output: [batch, seq_len, hidden_size]\n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        # attention_weights: [batch, seq_len, 1]\n",
    "        attention_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        \n",
    "        # context_vector: [batch, hidden_size]\n",
    "        context_vector = torch.sum(attention_weights * lstm_out, dim=1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        out = self.relu(self.fc1(context_vector))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        if return_attention:\n",
    "            return out, attention_weights\n",
    "        return out\n",
    "\n",
    "# Initialize model\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "\n",
    "model = LSTMWithAttention(\n",
    "    input_size=N_FEATURES,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    output_size=1,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "print(\"âœ“ Model initialized\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Define Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, device, train_loader, criterion, optimizer):\n",
    "    \"\"\"Train model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for sequence, labels in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "        sequence, labels = sequence.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sequence)\n",
    "        labels = labels.reshape(-1, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "    \n",
    "    return running_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate_model(model, device, data_loader, criterion, threshold=0.5):\n",
    "    \"\"\"Evaluate model on a dataset\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    y_probs = []\n",
    "    y_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequence, labels in data_loader:\n",
    "            sequence, labels = sequence.to(device), labels.to(device)\n",
    "            outputs = model(sequence)\n",
    "            labels = labels.reshape(-1, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "            \n",
    "            # Save predictions and labels\n",
    "            y_probs.append(outputs.cpu())\n",
    "            y_labels.append(labels.cpu())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    y_probs = torch.cat(y_probs).numpy().flatten()\n",
    "    y_labels = torch.cat(y_labels).numpy().flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    avg_loss = running_loss / len(data_loader.dataset)\n",
    "    y_pred = (y_probs > threshold).astype(int)\n",
    "    \n",
    "    precision = precision_score(y_labels, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_labels, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_labels, y_pred, zero_division=0)\n",
    "    \n",
    "    return avg_loss, precision, recall, f1, y_probs, y_labels\n",
    "\n",
    "print(\"âœ“ Training and evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 50\n",
    "PATIENCE = 7\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "# Initialize optimizer and loss function\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training loop with early stopping\n",
    "best_val_loss = float('inf')\n",
    "best_f1 = 0\n",
    "patience_counter = 0\n",
    "history = {\n",
    "    'train_loss': [], 'val_loss': [],\n",
    "    'val_precision': [], 'val_recall': [], 'val_f1': []\n",
    "}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Starting Training\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Train\n",
    "    train_loss = train_epoch(model, device, train_loader, criterion, optimizer)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, precision, recall, f1, _, _ = evaluate_model(\n",
    "        model, device, val_loader, criterion, THRESHOLD\n",
    "    )\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_precision'].append(precision)\n",
    "    history['val_recall'].append(recall)\n",
    "    history['val_f1'].append(f1)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Val Metrics - P: {precision:.4f} | R: {recall:.4f} | F1: {f1:.4f}\")\n",
    "    \n",
    "    # Early stopping based on F1 score\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        best_model_state = model.state_dict()\n",
    "        print(\"  âœ“ New best model!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  Patience: {patience_counter}/{PATIENCE}\")\n",
    "    \n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f\"\\nâœ“ Early stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(best_model_state)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Training completed! Best model loaded.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Final Evaluation on Test Set\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_loss, test_precision, test_recall, test_f1, test_probs, test_labels = evaluate_model(\n",
    "    model, device, test_loader, criterion, THRESHOLD\n",
    ")\n",
    "\n",
    "# Calculate AUC\n",
    "fpr, tpr, _ = roc_curve(test_labels, test_probs)\n",
    "test_auc = auc(fpr, tpr)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "test_pred = (test_probs > THRESHOLD).astype(int)\n",
    "cm = confusion_matrix(test_labels, test_pred)\n",
    "\n",
    "print(f\"\\nTest Set Results:\")\n",
    "print(f\"  Loss:      {test_loss:.4f}\")\n",
    "print(f\"  Precision: {test_precision:.4f}\")\n",
    "print(f\"  Recall:    {test_recall:.4f}\")\n",
    "print(f\"  F1 Score:  {test_f1:.4f}\")\n",
    "print(f\"  AUC-ROC:   {test_auc:.4f}\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"                Predicted\")\n",
    "print(f\"              No Churn  Churn\")\n",
    "print(f\"Actual No Churn  {cm[0,0]:4d}    {cm[0,1]:4d}\")\n",
    "print(f\"       Churn     {cm[1,0]:4d}    {cm[1,1]:4d}\")\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(test_labels, test_pred, target_names=['No Churn', 'Churn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training History\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0, 0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0, 0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=11)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision\n",
    "axes[0, 1].plot(history['val_precision'], label='Precision', color='blue', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Precision', fontsize=12)\n",
    "axes[0, 1].set_title('Validation Precision', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_ylim([0, 1])\n",
    "\n",
    "# Recall\n",
    "axes[1, 0].plot(history['val_recall'], label='Recall', color='green', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Recall', fontsize=12)\n",
    "axes[1, 0].set_title('Validation Recall', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_ylim([0, 1])\n",
    "\n",
    "# F1 Score\n",
    "axes[1, 1].plot(history['val_f1'], label='F1 Score', color='red', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 1].set_ylabel('F1 Score', fontsize=12)\n",
    "axes[1, 1].set_title('Validation F1 Score', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Training history plotted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve and Confusion Matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# ROC Curve\n",
    "axes[0].plot(fpr, tpr, label=f'ROC curve (AUC = {test_auc:.3f})', linewidth=3, color='#2E86AB')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=2)\n",
    "axes[0].set_xlabel('False Positive Rate', fontsize=12)\n",
    "axes[0].set_ylabel('True Positive Rate', fontsize=12)\n",
    "axes[0].set_title('ROC Curve - Churn Prediction', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Confusion Matrix\n",
    "im = axes[1].imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "axes[1].figure.colorbar(im, ax=axes[1])\n",
    "axes[1].set(xticks=[0, 1], yticks=[0, 1],\n",
    "            xticklabels=['No Churn', 'Churn'],\n",
    "            yticklabels=['No Churn', 'Churn'],\n",
    "            xlabel='Predicted Label',\n",
    "            ylabel='True Label',\n",
    "            title='Confusion Matrix')\n",
    "\n",
    "# Add text annotations\n",
    "thresh = cm.max() / 2\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        text = axes[1].text(j, i, f'{cm[i, j]}\\n({cm[i, j]/cm.sum()*100:.1f}%)',\n",
    "                           ha=\"center\", va=\"center\",\n",
    "                           color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                           fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ ROC curve and confusion matrix plotted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve\n",
    "precision_vals, recall_vals, thresholds_pr = precision_recall_curve(test_labels, test_probs)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(recall_vals, precision_vals, linewidth=3, color='#A23B72')\n",
    "ax.set_xlabel('Recall', fontsize=12)\n",
    "ax.set_ylabel('Precision', fontsize=12)\n",
    "ax.set_title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Precision-Recall curve plotted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Generate Predictions for All Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for test set\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in range(len(test_df)):\n",
    "        sequence, label = test_dataset[idx]\n",
    "        sequence = sequence.unsqueeze(0).to(device)  # Add batch dimension\n",
    "        \n",
    "        # Get prediction and attention weights\n",
    "        prob, attention = model(sequence, return_attention=True)\n",
    "        \n",
    "        test_predictions.append({\n",
    "            'account_id': test_df.iloc[idx]['account_id'],\n",
    "            'actual_churn': int(label.item()),\n",
    "            'churn_probability': float(prob.cpu().item()),\n",
    "            'predicted_churn': int((prob.cpu().item() > THRESHOLD)),\n",
    "            'sequence_length': int(test_df.iloc[idx]['seq_length'])\n",
    "        })\n",
    "\n",
    "# Create predictions DataFrame\n",
    "predictions_df = pd.DataFrame(test_predictions)\n",
    "\n",
    "print(f\"\\nâœ“ Generated predictions for {len(predictions_df):,} test accounts\")\n",
    "print(f\"\\nSample predictions:\")\n",
    "print(predictions_df.head(10))\n",
    "\n",
    "# Show high-risk accounts\n",
    "print(f\"\\n=== High Churn Risk Accounts (Probability > 0.7) ===\")\n",
    "high_risk = predictions_df[predictions_df['churn_probability'] > 0.7].sort_values('churn_probability', ascending=False)\n",
    "print(high_risk[['account_id', 'churn_probability', 'actual_churn', 'predicted_churn']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Results to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to Snowflake (MY_DATABASE.PUBLIC)\n",
    "print(\"\\nSaving predictions to MY_DATABASE.PUBLIC.CHURN_PREDICTIONS...\")\n",
    "\n",
    "try:\n",
    "    # Create Snowpark DataFrame from predictions\n",
    "    predictions_snowpark = session.create_dataframe(predictions_df)\n",
    "    \n",
    "    # Write to table in MY_DATABASE.PUBLIC\n",
    "    predictions_snowpark.write.mode(\"overwrite\").save_as_table(\"MY_DATABASE.PUBLIC.CHURN_PREDICTIONS\")\n",
    "    \n",
    "    print(f\"âœ“ Predictions saved to MY_DATABASE.PUBLIC.CHURN_PREDICTIONS table\")\n",
    "    print(f\"  Rows: {len(predictions_df):,}\")\n",
    "    \n",
    "    # Verify\n",
    "    result_count = session.table(\"MY_DATABASE.PUBLIC.CHURN_PREDICTIONS\").count()\n",
    "    print(f\"  Verified row count: {result_count:,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Error saving predictions: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model metrics to Snowflake (MY_DATABASE.PUBLIC)\n",
    "print(\"\\nSaving model metrics to MY_DATABASE.PUBLIC.CHURN_MODEL_METRICS...\")\n",
    "\n",
    "try:\n",
    "    # Create metrics DataFrame\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'model_name': ['LSTM_with_Attention'],\n",
    "        'train_date': [datetime.now()],\n",
    "        'test_loss': [test_loss],\n",
    "        'test_precision': [test_precision],\n",
    "        'test_recall': [test_recall],\n",
    "        'test_f1_score': [test_f1],\n",
    "        'test_auc_roc': [test_auc],\n",
    "        'num_features': [N_FEATURES],\n",
    "        'lookback_window': [MAX_LOOKBACK_WINDOW],\n",
    "        'hidden_size': [HIDDEN_SIZE],\n",
    "        'num_layers': [NUM_LAYERS],\n",
    "        'learning_rate': [LEARNING_RATE],\n",
    "        'train_samples': [len(train_df)],\n",
    "        'test_samples': [len(test_df)]\n",
    "    })\n",
    "    \n",
    "    # Save to Snowflake in MY_DATABASE.PUBLIC\n",
    "    metrics_snowpark = session.create_dataframe(metrics_df)\n",
    "    metrics_snowpark.write.mode(\"append\").save_as_table(\"MY_DATABASE.PUBLIC.CHURN_MODEL_METRICS\")\n",
    "    \n",
    "    print(f\"âœ“ Model metrics saved to MY_DATABASE.PUBLIC.CHURN_MODEL_METRICS table\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Error saving metrics: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CHURN PREDICTION MODEL - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nðŸ“Š Model Performance:\")\n",
    "print(f\"  F1 Score:   {test_f1:.4f}\")\n",
    "print(f\"  Precision:  {test_precision:.4f}\")\n",
    "print(f\"  Recall:     {test_recall:.4f}\")\n",
    "print(f\"  AUC-ROC:    {test_auc:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Data Statistics:\")\n",
    "print(f\"  Data source: MY_DATABASE.PUBLIC\")\n",
    "print(f\"  Total accounts processed: {len(sequence_df):,}\")\n",
    "print(f\"  Training samples: {len(train_df):,}\")\n",
    "print(f\"  Test samples: {len(test_df):,}\")\n",
    "print(f\"  Churn rate: {sequence_df['churn_label'].mean():.2%}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Model Configuration:\")\n",
    "print(f\"  Architecture: LSTM with Attention\")\n",
    "print(f\"  Features: {N_FEATURES}\")\n",
    "print(f\"  Lookback window: {MAX_LOOKBACK_WINDOW} months\")\n",
    "print(f\"  Hidden size: {HIDDEN_SIZE}\")\n",
    "print(f\"  Layers: {NUM_LAYERS}\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "print(f\"\\nðŸ’¾ Saved to Snowflake (MY_DATABASE.PUBLIC):\")\n",
    "print(f\"  CHURN_PREDICTIONS - Account-level predictions\")\n",
    "print(f\"  CHURN_MODEL_METRICS - Model performance metrics\")\n",
    "\n",
    "print(f\"\\nâœ… Next Steps:\")\n",
    "print(f\"  1. Query predictions: SELECT * FROM MY_DATABASE.PUBLIC.CHURN_PREDICTIONS WHERE CHURN_PROBABILITY > 0.7\")\n",
    "print(f\"  2. Monitor model performance over time\")\n",
    "print(f\"  3. Retrain model with new data periodically\")\n",
    "print(f\"  4. Integrate predictions into business workflows\")\n",
    "print(f\"  5. Consider feature engineering improvements\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ“ Churn Prediction Pipeline Complete!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
